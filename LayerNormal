什么是层归一化
层归一化是对数据进行处理的一种方式，把值按照公式缩放到一定的范围内，防止数据异常
为什么要用层归一化，对数据的单个样本友好，方便查看样本情况
为什么要在神经网络中用归一化，主要是用来稳定数据


简而言之就是，层归一化做了什么事情：
1，均值为0
2，方差为1
3，gamma进行缩放，beta进行平移


 均值为0，方差为1， gamma进行缩放，初始值为1，beta进行平移出始值为0
‌原理‌：
    *       ‌γ=1‌：初始不改变归一化后的方差，保留原始分布尺度；
    *       ‌β=0‌：初始不偏移归一化后的均值，维持中心位置。
优势‌：
    *       训练初期避免分布突变，梯度计算更稳定；
    *       作为基准起点，模型后续通过反向传播自主调整权重‌

   策略            场景                      训练稳定性           收敛速度
   γ=1, β=0    多数标准网络（Transformer等）    高                  稳定   ‌
   β=0.01      ReLU/LeakyReLU激活层           中→高               加快初期   ‌
  β=1~2        LSTM/GRU遗忘门                 高                  显著提升   ‌
   γ=0         残差网络跳跃连接                需配合调参            可能变慢    ‌

   结论
   γ‌：是特征的「‌放大器‌」，让模型自主决定哪些信息重要
‌   β‌：是分布的「‌定位器‌」，将数据精准匹配到激活函数高效区域

---------

  大白话就是：层归一化对数据做了一个标准化处理，分为3步
    第一步
        mean = sum(xx) / len(xx)，得到均值
    第二步
        std = sqrt(mean ** 2)，计算方差
    第三步
        gamma和bate
        gamma * std + bate
        gamma用来进行缩放，对某一个特征进行放大，缩小处理
        bate进行平移，对特征进行平移

以下架构适用于大多数智能问答和RAG检索系统
如转载请注明出处，也是我坚持原创技术分享的动力！

整体架构
=======================================================================
  业务层: 【智能问答】【RAG检索】【图片生成】【视频处理】【其他业务1 / 其他业务2 ...】
  中间层: 【Langchain】【LangGraph】【LlamaIndex】【Agent】【Workflow】
  核心层: 【Lora】【DPO】【向量数据库】【图数据库】【图片特征处理】【API管理】
  基础层: 【Qwen / DeepSeek / MOE 】【DDPM】【DALL】
  硬件层: 【4090 24G】【A100 80G】【H100 80G】【V100 48G】【L40 48G】
=======================================================================

对于这套系统，我大概分为5个层次，每个层次对应不同的功能

业务层面:场景大多都是 RAG检索和智能问答或者智能聊天，当然也有多模态的形式，比如要处理图片，生成视频等。
中间层:这里就涉及到技术相关的处理了，文本切割/DPF加载/Graph和任务处理，甚至中间件调用等逻辑，这层处理完以后，进入核心层
核心层:这里是我们对基础模型进行的微调后的内容，包含了微调/图数据库存储和向量检索等关键业务，关键意图识别等功能
基础层:基于市面上现有模型使用，如Qwen2/DeepSeek，如果对视觉有重要处理可以使用DDPM模型
硬件层:基于模型来进行使用什么卡比较合适，以qwen2--7B模型来参考，使用BF16浮点数，那么2个字节* 7B参数，那么大概占用14GB的显存

---------------------------------------------------------------------------------------------------------      
注意：14GB的显存这个只是加载模型需要的显卡，在实际使用中，需要根据用户量和QPS并发数来进行显存配置

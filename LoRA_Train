 注意：
	1，基于自己的业务来选择模型，如果市面上没有刚好合适的模型，那么就需要自己进行全量微调，这个是另外一个话题，我们这里只介绍LoRA！
	2，不同的模型有不同的协议，请仔细查看对应的License (开源协议):
		Apache 2.0 / MIT: 随便用，可商用。
		Llama Community: 可商用但在一定规模下有限制。
		CC-BY-NC: 仅限学术研究，不可商用。
	
 


    现在我们开始LoRA实战意图


   首先我们要选择一个模型，自己业务适合的模型，现在开源的大模型如雨后春笋一般层出不穷，不同的模型有不同的规则，我们可以通过命名来了解大概，比如Llama-3-70B-Instruct-GPTQ-Int4，
   这个命名的方式大概表示，[模型名称] - [参数大小] - [架构/特性] - [功能类型] - [量化/格式]，如果要更详细的查看模型的信息，我们需要查看模型的简介和config文件
  当我们选择了一个模型之后，紧接着，我们看一下模型提供的config文件来了解这个模型的信息。
	
    BaseModel的信息
        如何查看BaseModel信息，可以通过模型提供的config.json文件来进行查看，这里有个地方需要注意的是，每个模型的config.json文件中的key对应的value不一定都是一样的，但是有几个重要的key是一样的，
        config.json文件中，比较重要的信息有:
        architectures: ["Qwen2ForCausalLM"]，加载此模型时对应的类名。CausalLM代表 (Causal Language Model)
        model_type: "qwen2"，加载这个模型时模型的类型标识
        transformers_version: "4.40.1" 加载这个模型需要的transformers版本
        num_hidden_layers：24 层数，表示模型的Transformer Block堆叠的层数
        hidden_size: 4096隐藏层维度。这代表模型内部向量的宽度。
        vocab_size: 151936，词表大小。
        max_position_embeddings: 32768最大上下文长度。理论上这个模型支持，大约3万汉字
        rope_theta: 1000000.0，RoPE (旋转位置编码) 这是为了支持超长上下文（Long Context）而专门设置的。
        torch_dtype: "bfloat16"，权重精度。模型权重默认是以 bfloat16 格式存储的。
        hidden_act: "silu"，激活函数，使用的是 SiLU (Swish)


    数据加载
        当我们有了数据之后，不同的数据格式，需要不同的处理，我这里的BaseModel是CausalLM类型的模型，这种模型需要识别的是ChatML格式，这种格式是一种【数据--->指令---> 输出】的形式，其格式如下所示:
        <|im_start|>user{用户的指令}<|im_end|>
        <|im_start|>assistant{AI的回答}<|im_end|>
        <|im_start|>：告诉模型，“注意，这里开始一个新的角色发言了”。
        <|im_end|>：告诉模型，“这个角色的发言结束了”。
        如果不添加这种ChatML的格式，BaseModel就无法区分哪部分是输入的指令，哪部分是它应该学习的回答。它可能会混淆上下文，甚至在推理时不知道什么时候该停止生成


    3，模型训练
	现在我们BaseModel模型准备好了，数据也处理好了之后，我们开始训练模型，LoRA(一)LoRA的原理，当中介绍了LoRA公式和训练的目的，如果忘记的话，请重新读一遍，这里我们通过transfomers、peft、datasets库来进行训练。
	1，首先要通过load_dataset加载自己准备好的数据，加载后并进行AutoTokenizer
	2，使用AutoModelForCausalLM加载BaseModel，并指定使用哪个设备进行训练
	3，设置LoraConfig，包括设置rank以及alpha，线性层等等参数，最后对BaseModel应用这个配置，如：get_peft_model(model, lora_config)
	4，设置TrainingArguments，这里是训练的参数，LoRA(一）对参数进行了介绍（或查看图片中的代码示例）
	5，进行Trainer，这里是把之前的配置和数据以及BaseModel进行训练，并且设置最后合并的模型存储的位置
	6，最后，查看loss的信息，是否符合预期。



    4，查看意图
	模型训练后，我们开始进行验证，比如我们要查看模型是否真正的懂了意图，这里我们不只看模型生成的句子通不通顺，更要看它是否输出了正确的分类标签或指令格式。
	假设我们的训练目标是让模型识别“退款”和“查询物流”两个意图，我们对训练后的模型输入问题：“我的快递到哪了？”
	这里我们有个假设期望输出：{"intent": “意图_逻辑处理_回答”}
	这里如果模型直接输出：“好的，我帮您查一下。”（虽然通顺，但不是结构化的意图，属于失败）
	这里我们期望模型输出内容是：“好的，我发现了两条快递信息，我帮您查一下。查询的内容是：第一条是xx快递。第二条是xx快递”（能够理解意图）

	那么如果让模型能理解更准确的意图，目前我的经验是有几种方式
	1，从准备的数据中获取，也就是我们目前的情况check_logistics
	2，对数据进行进行处理，添加更多关联，比如：思维链CoT，标签Tagging等方式
	3，Rank的参数设置，Alpha的参数设置等，以及训练的轮次
	4，是否需要调用对应服务，比如使用MCP等
	5，根据业务需要是否使用Rag或者图数据库等
	6，根据我在github.com/AI-QKV（提过的模块优化公式来进行）
	7，其他方式等



    5，最后的结语
正如在之前的文章中所提到的，掌握原理只是前提，真正的挑战在于面对复杂业务场景时的调试与优化。
模型训练不是一蹴而就的，它需要我们根据 Loss 的曲线、准确率的反馈，不断调整参数，希望这篇实战指南能帮助大家在自己的业务中，用最小的成本（LoRA），训练出最懂用户“心意”的模型。持续迭代，才是通向高性能企业级 LLM 的最终路径
    

LoRA深入浅出

---------------------------
LoRA原理
在众多训练模型的方法中，高效微调参数是绕不过的话题。
那么高效微调参数，为什么有那么大的魔力，是因为，高效微调参数，它用轻量级的参数，实现了接近全量微调的性能效果，同时显著降低了计算成本、存储需求和训练时间‌。
而其中LoRA又在这方面又具有独特的优势

LoRA公式
'w = w + a/r (AB)

说明：
'w 表示的是训练后的模型，也就是 new模型
w 是base Model，原始模型，基础模型，这个取决你基于什么模型来进行
r 是rank，论文建议是大于8，小于64，超过128 丧失高效性且可能过拟合
a 是，alpha表示缩放因子，用来控制LoRA的矩阵参数的权重，论文中的建议 alpha=2r或alpha=r
AB是LoRA中的矩阵，A(input, r), B(r, output)
以上内容就是LoRA的公式。

LoRA及模型级别
那么究竟如何使用LoRA进行训练呢？在LoRA当中，其原理是对模型的target_modules进行替换或者包装，它并不会直接修改比如q_proj,v_proj层的内容（如果不熟悉target_modules，请先去学习Transformers）。
对于不同的模型，有不同的规则，比如我们按下面的参数量来进行
LLM大小为10B左右，rank的起始可以从4开始，也可以从8开始，这种是适用于大多数任务，尤其是在计算预算有限的情况下使用。
LLM大小为32B左右，可以从16开始，论文中的建议是8起步，64以内是最佳的效果，所以对于32B模型来说，要达到性能和参数量之间良好的平衡，需要设置比8更高的rank，所以这里从16开始也是符合规范的
LLM大小为64B左右，可以从24开始，且32开始也可以接受，根据微调效果，来达到性能和参数量之间的良好平衡点。


LoRA和数据集
数据集，对于不同的模型，需要的数据也不相同，如下参考
LLM大小为10B左右，3000条或者5000条高质量数据。这种量级的数据，可以进行角色扮演/知识注入/简单分类。
LLM大小为32B左右，5000条或者1万条高质量数据，建议1万条。可以进行复杂推理/多轮对话/专业QA。
LLM大小为64B左右，1万条或者2万条高质量数据，这种量级数据，可以进行企业级应用/跨领域决策。


影响LoRA的因素
在微调过程中，除了rank还有哪些因素会影响
缩放因子(a也称为alpha) 它的作用控制参数权重。
如果这个值过小，会导致梯度失衡，破坏训练的稳定性，无法有效学习新任务特征。
如果这个值过大，压制原始模型的知识，出现影响baseModel原来的权重，造成预训练知识覆盖和新任务过拟合，降低泛化能力‌导致学的不够准确

LR学习率，优化步长。控制参数更新的快慢
学习率的本质是这样的，预测的结果和真实的结果之间的差异，如果预测值和真实值一致，那么这个学习率就不用调整。
如果预测值和真实值有所不同，才需要调整学习率，合适的值能帮助模型高效、稳定地找到最优解‌

优化器，优化算法。决定如何使用梯度更新参数。常见的有，SGD，AdamW，预热cosine等等，那么在LoRA中应该用那个，目前常用的是AdamW
LoRA参数比较少，AdamW能更高效地平衡新旧知识。
